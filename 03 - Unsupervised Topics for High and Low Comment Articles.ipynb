{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook I'll be creating topics for the high-comment articles and low-comment articles.  My most successful modeling for this was with CountVec LDA, but I'll also demonstrate my modeling when done with TFIDF LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_csv(\"CleanedApril2018.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We're going to start by looking at low-comment articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nocom=df[df.Outcome != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/JonathonBowyer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "stopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopword.union(set(['trump', 'donald', 'us', 'politics', 'j', 'united', 'states', \"new\", \"york\", \"inc\", \"ny\", \"city\", \"manhattan\", \"government\", 'nyc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  chapter_reader_nocom():\n",
    "    for i in df_nocom['keywords']:\n",
    "        yield (x for x in \n",
    "            gensim.utils.tokenize(i, lowercase=True, deacc=True, \n",
    "                                  errors=\"ignore\")\n",
    "            if x not in stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_nocom = []\n",
    "dictionary = gensim.corpora.Dictionary(chapter_reader_nocom())\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8, keep_n=100000)\n",
    "for values in chapter_reader_nocom():\n",
    "    corpus_nocom.append(dictionary.doc2bow(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.015*\"tv\" + 0.015*\"program\" + 0.008*\"television\" + 0.006*\"art\" + 0.005*\"estate\" + 0.005*\"puzzles\" + 0.005*\"crossword\" + 0.005*\"real\" + 0.005*\"housing\" + 0.005*\"photography\"'),\n",
       " (1,\n",
       "  '0.007*\"housing\" + 0.007*\"international\" + 0.007*\"real\" + 0.007*\"estate\" + 0.006*\"relations\" + 0.006*\"residential\" + 0.006*\"state\" + 0.005*\"elections\" + 0.005*\"play\" + 0.005*\"news\"'),\n",
       " (2,\n",
       "  '0.012*\"theater\" + 0.006*\"play\" + 0.005*\"women\" + 0.005*\"girls\" + 0.004*\"crimes\" + 0.004*\"book\" + 0.004*\"program\" + 0.004*\"estate\" + 0.004*\"tv\" + 0.004*\"education\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_nocom = gensim.models.LdaModel(corpus_nocom, id2word=dictionary, num_topics=3, random_state=23)\n",
    "lda_nocom.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we're going to look at high-comment articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com=df[df.Outcome == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/JonathonBowyer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "stopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopword.union(set(['trump', 'donald', 'us', 'politics', 'j', 'united', 'states', \"new\", \"york\", \"inc\", \"ny\", \"city\", \"manhattan\", \"government\", 'nyc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  chapter_reader_com():\n",
    "    for i in df_com['keywords']:\n",
    "        yield (x for x in \n",
    "            gensim.utils.tokenize(i, lowercase=True, deacc=True, \n",
    "                                  errors=\"ignore\")\n",
    "            if x not in stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_com = []\n",
    "dictionary = gensim.corpora.Dictionary(chapter_reader_com())\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8, keep_n=100000)\n",
    "for values in chapter_reader_com():\n",
    "    corpus_com.append(dictionary.doc2bow(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"elections\" + 0.012*\"party\" + 0.007*\"republican\" + 0.006*\"state\" + 0.006*\"ties\" + 0.006*\"democratic\" + 0.006*\"national\" + 0.006*\"house\" + 0.005*\"international\" + 0.005*\"associates\"'),\n",
       " (1,\n",
       "  '0.007*\"international\" + 0.006*\"elections\" + 0.005*\"house\" + 0.005*\"jr\" + 0.004*\"news\" + 0.004*\"control\" + 0.004*\"representatives\" + 0.004*\"federal\" + 0.004*\"department\" + 0.004*\"syria\"'),\n",
       " (2,\n",
       "  '0.014*\"international\" + 0.011*\"relations\" + 0.007*\"defense\" + 0.007*\"elections\" + 0.007*\"interference\" + 0.007*\"department\" + 0.007*\"associates\" + 0.006*\"russian\" + 0.006*\"military\" + 0.006*\"ties\"')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_com = gensim.models.LdaModel(corpus_com, id2word=dictionary, num_topics=3, random_state=5)\n",
    "lda_com.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we'll do the same, except using TFIDF.  We'll start with low-comment articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  chapter_reader_nocom():\n",
    "    for i in df_nocom['keywords']:\n",
    "        yield (x for x in \n",
    "            gensim.utils.tokenize(i, lowercase=True, deacc=True, \n",
    "                                  errors=\"ignore\")\n",
    "            if x not in stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_nocom = []\n",
    "dictionary = gensim.corpora.Dictionary(chapter_reader_nocom())\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8, keep_n=100000)\n",
    "for values in chapter_reader_nocom():\n",
    "    corpus_nocom.append(dictionary.doc2bow(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus_nocom, normalize=True)\n",
    "corpus_nocom_tfidf = tfidf[corpus_nocom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"television\" + 0.006*\"tv\" + 0.006*\"program\" + 0.003*\"billions\" + 0.002*\"city\" + 0.002*\"australia\" + 0.002*\"trust\" + 0.002*\"traffic\" + 0.002*\"upper\" + 0.002*\"inc\"'),\n",
       " (1,\n",
       "  '0.005*\"estate\" + 0.005*\"real\" + 0.005*\"housing\" + 0.005*\"residential\" + 0.004*\"nyc\" + 0.004*\"program\" + 0.004*\"tv\" + 0.003*\"travel\" + 0.003*\"vacations\" + 0.003*\"television\"'),\n",
       " (2,\n",
       "  '0.004*\"theater\" + 0.004*\"books\" + 0.004*\"literature\" + 0.004*\"puzzles\" + 0.004*\"crossword\" + 0.003*\"rights\" + 0.003*\"play\" + 0.002*\"inc\" + 0.002*\"jr\" + 0.002*\"civil\"'),\n",
       " (3,\n",
       "  '0.003*\"education\" + 0.003*\"life\" + 0.003*\"city\" + 0.003*\"real\" + 0.003*\"estate\" + 0.003*\"housing\" + 0.003*\"program\" + 0.003*\"tv\" + 0.003*\"family\" + 0.002*\"families\"'),\n",
       " (4,\n",
       "  '0.007*\"puzzles\" + 0.006*\"crossword\" + 0.003*\"international\" + 0.003*\"trade\" + 0.003*\"china\" + 0.002*\"world\" + 0.002*\"market\" + 0.002*\"france\" + 0.002*\"theater\" + 0.002*\"vacations\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_nocom = gensim.models.LdaModel(corpus_nocom_tfidf, id2word=dictionary, num_topics=5, random_state=0)\n",
    "lda_nocom.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF for high-comment articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  chapter_reader():\n",
    "    for i in df_com['keywords']:\n",
    "        yield (x for x in \n",
    "            gensim.utils.tokenize(i, lowercase=True, deacc=True, \n",
    "                                  errors=\"ignore\")\n",
    "            if x not in stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "dictionary = gensim.corpora.Dictionary(chapter_reader())\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8, keep_n=100000)\n",
    "for values in chapter_reader():\n",
    "    corpus.append(dictionary.doc2bow(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus, normalize=True)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"party\" + 0.004*\"republican\" + 0.003*\"defense\" + 0.003*\"military\" + 0.003*\"forces\" + 0.003*\"iii\" + 0.003*\"elections\" + 0.003*\"intelligence\" + 0.003*\"robert\" + 0.003*\"rights\"'),\n",
       " (1,\n",
       "  '0.006*\"puzzles\" + 0.006*\"crossword\" + 0.004*\"elections\" + 0.003*\"interference\" + 0.003*\"russian\" + 0.003*\"associates\" + 0.003*\"ties\" + 0.003*\"syria\" + 0.003*\"inc\" + 0.003*\"cohen\"'),\n",
       " (2,\n",
       "  '0.006*\"immigration\" + 0.003*\"ryan\" + 0.003*\"representatives\" + 0.003*\"international\" + 0.003*\"house\" + 0.003*\"paul\" + 0.003*\"illegal\" + 0.003*\"party\" + 0.003*\"emigration\" + 0.003*\"jr\"'),\n",
       " (3,\n",
       "  '0.005*\"elections\" + 0.005*\"crossword\" + 0.005*\"puzzles\" + 0.004*\"international\" + 0.004*\"trade\" + 0.003*\"party\" + 0.003*\"book\" + 0.003*\"house\" + 0.003*\"inc\" + 0.003*\"market\"'),\n",
       " (4,\n",
       "  '0.004*\"international\" + 0.003*\"federal\" + 0.003*\"relations\" + 0.003*\"news\" + 0.003*\"syria\" + 0.003*\"defense\" + 0.003*\"media\" + 0.003*\"elections\" + 0.003*\"trade\" + 0.003*\"e\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=5, random_state=4)\n",
    "lda.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
